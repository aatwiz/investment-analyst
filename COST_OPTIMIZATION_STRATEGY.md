# Cost & Performance Optimization Strategy

## ðŸš¨ Your Concerns (100% Valid!)

> "Given only raw text will be analysed won't the token consumption become much much higher? How will this translate later on when we want to use segmented searches for inquiries about data and whatnot, I feel this will become expensive quite quickly will it not? And how will speed be affected?"

**Short answer:** YES, you're absolutely right! The current implementation is inefficient. Here's the solution.

---

## ðŸ“Š Current Architecture Analysis

### Two Separate Workflows Already Exist

#### 1. **Initial Analysis** (Expensive - ONE-TIME per document)
```
Document Upload â†’ Full Document Analysis â†’ Generate Summary
â†“
InvestmentAnalystAgent
- Reads up to 50K characters of raw text
- Cost: ~$0.02-0.05 per document
- Speed: ~10-30 seconds
- Frequency: ONCE per document
```

#### 2. **Semantic Search** (Cheap - MANY TIMES)
```
User Query â†’ Vector Search â†’ Retrieve Relevant Chunks â†’ Answer
â†“
Already chunked in database!
- Uses 500-char chunks (already implemented)
- Cost: ~$0.001-0.005 per query
- Speed: ~1-3 seconds
- Frequency: HUNDREDS of times
```

### Good News: You Already Have Chunking!

Looking at `backend/services/embeddings/embedding_service.py`:

```python
CHUNK_SIZE = 500  # Characters per chunk
CHUNK_OVERLAP = 50  # Overlap between chunks

async def embed_document_chunks(
    session: AsyncSession,
    document_id: int,
    full_text: str,
    chunk_size: int = CHUNK_SIZE,
    chunk_overlap: int = CHUNK_OVERLAP
) -> int:
    """
    Split document into chunks and generate embeddings for each.
    """
    chunks = split_text(full_text, chunk_size, chunk_overlap)
    # Generate embeddings for all chunks
    # Store in document_embeddings table
```

**This is perfect for queries!** âœ…

---

## ðŸ’° Cost Breakdown

### Current Costs (Actual Calculation)

| Operation | Model | Input Tokens | Output Tokens | Cost |
|-----------|-------|--------------|---------------|------|
| **Initial Analysis** (50K chars) | GPT-4o-mini | ~12,500 | ~1,000 | $0.0025 |
| **Query (RAG)** (5 chunks) | GPT-4o-mini | ~1,250 | ~500 | $0.0003 |

**GPT-4o-mini Pricing:**
- Input: $0.150 per 1M tokens
- Output: $0.600 per 1M tokens

**Example: 100 documents, 1000 queries/month**
- Initial analysis: 100 Ã— $0.0025 = **$0.25**
- Queries: 1000 Ã— $0.0003 = **$0.30**
- **Total: $0.55/month** (extremely cheap!)

### The Problem: If We Sent Full Docs for Every Query

| Operation | Cost | Monthly (1000 queries) |
|-----------|------|------------------------|
| Full document per query (50K chars) | $0.0025 | **$2.50** |
| Chunked RAG per query (5 chunks) | $0.0003 | **$0.30** |

**Savings: 8x cheaper with chunking!** ðŸŽ¯

---

## âœ… Recommended Architecture (Best of Both Worlds)

### 1. Initial Comprehensive Analysis (Current - Keep It!)

**Use Case:** First-time document upload
**Purpose:** Generate executive summary, risk assessment, investment thesis
**Method:** Feed full/large text to LLM
**Frequency:** ONCE per document
**Cost:** $0.02-0.05 per document
**Speed:** 10-30 seconds

```python
# backend/services/llm_agents/investment_analyst_agent.py
async def analyze_document(file_path: str, metadata: dict):
    """
    ONE-TIME comprehensive analysis for new documents.
    Expensive but necessary for quality insights.
    """
    raw_text = self.file_processor.process_file(file_path)
    
    # Truncate to reasonable size (50K chars)
    text_for_analysis = raw_text[:50000]
    
    # Get comprehensive LLM analysis
    analysis = await self._get_llm_insights(text_for_analysis)
    
    # Cache results in database
    await save_analysis_to_db(analysis)
    
    return analysis
```

**Why this is acceptable:**
- âœ… Happens only once per document
- âœ… Provides high-quality initial assessment
- âœ… Results cached in database
- âœ… Worth the cost for accuracy

### 2. Query-Time RAG (Implement This!)

**Use Case:** User asks questions about documents
**Purpose:** Answer specific questions, find relevant info
**Method:** Vector search â†’ retrieve relevant chunks â†’ answer
**Frequency:** MANY times per document
**Cost:** $0.0003-0.001 per query
**Speed:** 1-3 seconds

```python
# NEW: backend/services/llm_agents/rag_agent.py
class RAGQueryAgent:
    """
    Efficient agent for answering queries using vector search + LLM.
    Uses chunked data from document_embeddings table.
    """
    
    async def answer_query(
        self,
        query: str,
        document_ids: Optional[List[int]] = None,
        max_chunks: int = 5
    ) -> dict:
        """
        Answer user query using RAG (Retrieval-Augmented Generation).
        
        This is 8x cheaper than analyzing full documents!
        """
        # 1. Vector search to find relevant chunks
        relevant_chunks = await vector_search(
            session=self.session,
            query_text=query,
            limit=max_chunks,
            document_ids=document_ids
        )
        
        # 2. Build prompt with ONLY relevant chunks (~2.5K chars)
        context = "\n\n".join([
            f"[Source: {chunk['document_name']}]\n{chunk['content']}"
            for chunk in relevant_chunks
        ])
        
        prompt = f"""
        You are an investment analyst. Answer the following question based ONLY on the provided context.
        
        Question: {query}
        
        Context:
        {context}
        
        Answer:
        """
        
        # 3. LLM answers using minimal tokens
        response = await self.client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[{"role": "user", "content": prompt}],
            temperature=0.3,
            max_tokens=500  # Short answers only
        )
        
        return {
            "answer": response.choices[0].message.content,
            "sources": [chunk['document_id'] for chunk in relevant_chunks],
            "cost": calculate_cost(response.usage)
        }
```

**Benefits:**
- âœ… 8x cheaper than full-doc analysis
- âœ… Much faster (1-3 seconds)
- âœ… More focused answers
- âœ… Scales to thousands of queries

---

## ðŸš€ Optimization Techniques

### 1. Smart Truncation (Already Implemented!)

```python
# Current implementation in investment_analyst_agent.py
max_chars = 50000  # ~12,500 tokens
text_preview = raw_text[:max_chars]
```

**Cost savings:** 
- Full 100-page document = ~250K chars = ~60K tokens = $0.009
- Truncated 50K chars = ~12.5K tokens = $0.002
- **Savings: 78%!**

### 2. Tiered Analysis System (Recommended)

```python
# NEW: Different analysis depths for different needs
class AnalysisType(Enum):
    QUICK = "quick"          # 10K chars, $0.001, 5 sec
    STANDARD = "standard"     # 25K chars, $0.003, 15 sec
    COMPREHENSIVE = "comprehensive"  # 50K chars, $0.005, 30 sec
    DEEP_DIVE = "deep_dive"   # 100K chars, $0.015, 60 sec

async def analyze_document(
    file_path: str,
    analysis_type: AnalysisType = AnalysisType.STANDARD
):
    """
    Let users choose analysis depth based on needs.
    """
    max_chars = {
        AnalysisType.QUICK: 10000,
        AnalysisType.STANDARD: 25000,
        AnalysisType.COMPREHENSIVE: 50000,
        AnalysisType.DEEP_DIVE: 100000
    }[analysis_type]
    
    text_preview = raw_text[:max_chars]
    # ... rest of analysis
```

### 3. Caching Strategy (Critical!)

```python
# backend/services/llm_agents/investment_analyst_agent.py
async def analyze_document(self, file_path: str, force_reanalyze: bool = False):
    """
    Check cache before expensive LLM call.
    """
    # Check if analysis already exists
    if not force_reanalyze:
        cached = await get_cached_analysis(file_path)
        if cached:
            logger.info(f"Using cached analysis for {file_path}")
            return cached  # Cost: $0!
    
    # Only call LLM if not cached
    analysis = await self._get_llm_insights(prompt)
    await save_analysis_to_db(analysis)
    
    return analysis
```

**Cost savings:**
- First analysis: $0.025
- Subsequent views: $0.00 (cached)
- **100% savings on repeat requests!**

### 4. Batch Processing (For Multiple Documents)

```python
async def analyze_batch(document_paths: List[str]):
    """
    Process multiple documents efficiently.
    """
    # Process all files in parallel
    tasks = [analyze_document(path) for path in document_paths]
    results = await asyncio.gather(*tasks)
    
    return results
```

### 5. Streaming Responses (Better UX)

```python
async def analyze_document_stream(file_path: str):
    """
    Stream analysis results as they're generated.
    User sees results immediately, better perceived performance.
    """
    stream = await self.client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[{"role": "user", "content": prompt}],
        stream=True  # Stream tokens as they arrive
    )
    
    async for chunk in stream:
        yield chunk.choices[0].delta.content
```

---

## ðŸ“ˆ Performance Optimization

### Speed Comparison

| Operation | Current | Optimized | Improvement |
|-----------|---------|-----------|-------------|
| Initial Analysis | 30s | 30s | - (necessary) |
| Query (full doc) | 30s | - | - |
| Query (RAG) | - | 2s | **15x faster** |
| Cached Query | - | 0.1s | **300x faster** |

### Parallelization

```python
# Process multiple queries simultaneously
async def batch_query(queries: List[str]):
    tasks = [rag_agent.answer_query(q) for q in queries]
    results = await asyncio.gather(*tasks)
    return results
```

---

## ðŸŽ¯ Recommended Implementation Plan

### Phase 1: Immediate (Keep Current System)
âœ… **Already done!** Initial analysis works well for new documents.

### Phase 2: Add RAG Agent (PRIORITY - This Week)
1. Create `backend/services/llm_agents/rag_agent.py`
2. Implement `RAGQueryAgent` class
3. Add endpoint: `POST /api/v1/query/ask`
4. Use existing chunked embeddings from `document_embeddings` table

```python
# NEW ENDPOINT: backend/api/routes/query.py
@router.post("/ask")
async def ask_question(
    question: str,
    document_ids: Optional[List[int]] = None,
    session: AsyncSession = Depends(get_session)
):
    """
    Ask questions about documents using efficient RAG.
    Cost: $0.0003 per query (vs $0.025 for full analysis)
    """
    rag_agent = RAGQueryAgent(session)
    answer = await rag_agent.answer_query(question, document_ids)
    return answer
```

### Phase 3: Add Caching (Next Week)
1. Check database for existing analyses before LLM call
2. Add `force_reanalyze` parameter to API
3. Set cache expiration (e.g., 30 days)

### Phase 4: Add Tiered Analysis (Optional)
1. Add `analysis_depth` parameter to analysis endpoint
2. Let users choose: quick/standard/comprehensive/deep
3. Price accordingly in UI

---

## ðŸ’¡ Cost Projections (Real World)

### Scenario: Investment Firm with 1,000 Documents

**Month 1 (Initial Upload):**
- 1,000 comprehensive analyses: 1,000 Ã— $0.025 = **$25**
- 5,000 queries (RAG): 5,000 Ã— $0.0003 = **$1.50**
- **Total: $26.50**

**Month 2+ (Ongoing):**
- 100 new documents: 100 Ã— $0.025 = **$2.50**
- 10,000 queries (RAG): 10,000 Ã— $0.0003 = **$3.00**
- Cached analyses: **$0**
- **Total: $5.50/month**

**Annual Cost: ~$91** for a system handling 10,000+ queries! ðŸŽ‰

### Compare to Alternatives

| Solution | Monthly Cost | Notes |
|----------|--------------|-------|
| Current (optimized) | $5.50 | With RAG + caching |
| Full-doc every query | $250 | 50x more expensive! |
| GPT-4 (standard) | $150 | 27x more expensive |
| Claude Opus | $225 | 40x more expensive |

---

## ðŸ”§ Monitoring & Controls

### Add Cost Tracking

```python
# backend/models/query_log.py
class QueryLog(Base):
    __tablename__ = "query_logs"
    
    id = Column(Integer, primary_key=True)
    query_text = Column(Text)
    tokens_used = Column(Integer)
    cost_usd = Column(Numeric(10, 6))
    response_time_ms = Column(Integer)
    cache_hit = Column(Boolean)
    created_at = Column(DateTime, default=datetime.utcnow)
```

### Set Budget Limits

```python
# backend/config/settings.py
class LLMBudgetConfig:
    DAILY_BUDGET_USD = 10.0
    MONTHLY_BUDGET_USD = 300.0
    ALERT_THRESHOLD = 0.8  # Alert at 80% usage
    
    @classmethod
    async def check_budget(cls):
        usage = await get_monthly_usage()
        if usage > cls.MONTHLY_BUDGET_USD * cls.ALERT_THRESHOLD:
            await send_alert(f"LLM budget at {usage/cls.MONTHLY_BUDGET_USD:.0%}")
```

---

## ðŸ“‹ Summary

### Your Concerns Addressed

| Concern | Solution | Status |
|---------|----------|--------|
| High token consumption | RAG with chunked data | âœ… 8x cheaper |
| Expensive for queries | Vector search first | âœ… $0.0003/query |
| Speed issues | Cached results + RAG | âœ… 15x faster |
| Scalability | Two-tier architecture | âœ… Ready |

### Architecture Decision

**âœ… Keep:** Full-document analysis for initial uploads (accuracy worth the cost)

**âœ… Add:** RAG agent for all queries (8x cheaper, 15x faster)

**âœ… Implement:** Caching for repeat requests (100% savings)

### Next Steps

1. **This week:** Implement `RAGQueryAgent` for queries
2. **Test:** Compare costs (full-doc vs RAG)
3. **Monitor:** Track token usage and costs
4. **Optimize:** Add caching and tiered analysis

### Bottom Line

**You're right to be concerned, but the solution is straightforward:**
- Initial analysis: Expensive but necessary (ONCE per doc)
- All queries: Use cheap RAG (MANY times per doc)
- Cache everything: Free repeat requests

**Expected costs:** $5-10/month for a busy investment firm. Totally affordable! ðŸ’°âœ…

